<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" href="bootstrap/css/bootstrap.min.css">
    <link rel="stylesheet" href="style.css">
    <script src="https://kit.fontawesome.com/3aef636b73.js" crossorigin="anonymous"></script>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Latency Matters: Real-Time Action Forecasting Transformer</title>
</head>
<body class="h-100 d-flex flex-column">
    <main class="container flex-grow-1">
        <header class="py-3">
            <h2 class="custom-title mt-3">
                Latency Matters: Real-Time Action Forecasting Transformer
            </h2>
            <div class="authors custom-text mt-3">
                <div class="px-2">
                    <a href="https://www.linkedin.com/in/harshayu-girase-764b06153">Harshayu Girase</a>
                    <span><sup>1,2</sup>*</span>
                </div>
                <div class="px-2">
                    <a href="https://lukan94.github.io/">Nakul Agarwal</a>
                    <sup>1</sup>
                </div>
                <div class="px-2">
                    <a href="https://chihochoi.github.io/">Chiho Choi</a>
                    <sup>1</sup>
                </div>
                <div class="px-2">
                    <a href="https://karttikeya.github.io/">Karttikeya Mangalam</a>
                    <span><sup>2</sup>*</span>
                </div>
            </div>
            <div class="mt-1">
                <p><small>* denotes equal technical contribution</small></p>
            </div>
            <div class="insts custom-text mt-1">
                <div class="px-2"><sup>1</sup>Honda Research Institute USA</div>
                <div class="px-2"><sup>2</sup>UC Berkeley</div>
            </div>
            <div class="mt-4 mb-2">
                <span class="mx-1">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Girase_Latency_Matters_Real-Time_Action_Forecasting_Transformer_CVPR_2023_paper.pdf" class="btn btn-dark rounded-pill">
                      <i class="fas fa-file-pdf"></i>
                      <span>Paper</span>
                    </a>
                </span>
                <span class="mx-1">
                    <a href="https://www.youtube.com/watch?v=0tbK36hzSM0" class="btn btn-dark rounded-pill">
                      <i class="fab fa-youtube"></i>
                      <span>Video</span>
                    </a>
                </span>
                <span class="mx-1">
                    <a href="" class="btn btn-dark rounded-pill">
                      <i class="fab fa-github"></i>
                      <span>Code</span>
                    </a>
                </span>
                <span class="mx-1">
                    <a href="" class="btn btn-dark rounded-pill">
                      <i class="fas fa-chalkboard"></i>
                      <span>Slides</span>
                    </a>
                </span>
                <span class="mx-1">
                    <a href="" class="btn btn-dark rounded-pill">
                      <i class="far fa-clipboard"></i>
                      <span>Poster</span>
                    </a>
                </span>
            </div>
        </header>
        <div style="max-width:70%;margin:auto">
            <section id="new">
                <h3 class="text-center mt-4 mb-2">
                    Bigger Models are <u>NOT</u> Necessarily Better
                </h3>
                <p class="text-justify">
                    In this paper, we propose a new real-time setting for evaluating action forecasting networks where bigger is not necessarily better.
                    We propose a latency-aware real-time evaluation setting that better mimics practical deployment 
                    settings for embodied forecasting systems. Real-time evaluation demonstrates a clear trade-off between 
                    inference latency and model forecasting fidelity, paving the path for the development of latency-aware 
                    forecasting models in the future.
                </p>
                <div class="row px-1 my-2">
                    <div class="col"></div>
                    <div class="border custom-rounded col-7"><img src="images/tradeoff.png" alt="" style="max-width:100%"></div>
                    <div class="col"></div>
                </div>
                <p class="text-justify">
                    The graph on the right shows the trade-off between latency and high-fidelity forecasts in the real-time evaluation 
                    setting. Bigger models continue to perform better in latency agnostic offline settings. When latency is accounted 
                    for, bigger models with higher latency drop in forecasting performance because the model is unable to access more  
                    recent data.
                </p>
                <div class="row px-1 my-2">
                    <div class="col"></div>
                    <div class="border custom-rounded col-6"><img src="images/real-time-eval.png" alt="" style="max-width:100%"></div>
                    <div class="col"></div>
                </div>
                <p class="text-justify">
                    The diagram on the left details the difference between offline and real-time evaluation. During offline evaluation, 
                    inference latency is assumed to be zero, allowing predictions to be made at any time step using all prior frames up to the 
                    forecasting horizon. During real-time evaluation, the nonzero inference latency of the model must be accounted for. In this 
                    setting, predictions must be made using frames further in the past. The reduced latency of RAFTformer allows it to use 
                    significantly more recent frames than prior methods.
                </p>
            </section>
            <section id="video">
                <h3 class="text-center mt-4 mb-2">
                    Video
                </h3>
                <div class="row px-1 my-2">
                    <div class="col"></div>
                    <div class="border-0 custom-rounded embed-responsive embed-responsive-16by9 col-10">
                        <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/0tbK36hzSM0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </div>
                    <div class="col"></div>
                </div>
            </section>
            <section id="abstract">
                <h3 class="text-center mt-4 mb-2">
                    Abstract
                </h3>
                <p class="text-justify">
                    We present RAFTformer, a real-time action forecasting transformer for latency-aware real-world action forecasting. 
                    RAFTformer is a two-stage fully transformer based architecture comprising of a video transformer backbone that 
                    operates on high resolution, short-range clips, and a head transformer encoder that temporally aggregates 
                    information from multiple short-range clips to span a long-term horizon. Additionally, we propose a novel 
                    self-supervised shuffled causal masking scheme as a model level augmentation to improve forecasting fidelity. 
                    Finally, we also propose a novel real-time evaluation setting for action forecasting that directly couples model 
                    inference latency to overall forecasting performance and brings forth a hitherto overlooked trade-off between 
                    latency and action forecasting performance. Our parsimonious network design facilitates RAFTformer inference latency 
                    to be 9x smaller than prior works at the same forecasting accuracy. Owing to its two-staged design, RAFTformer uses 
                    94% less training compute and 90% lesser training parameters to outperform prior state-of-the-art baselines by 4.9 
                    points on EGTEA Gaze+ and by 1.4 points on EPIC-Kitchens100 validation set, as measured by Top-5 recall (T5R) in the 
                    offline setting. In the real-time setting, RAFTformer outperforms prior works by an even greater margin of up to 4.4 
                    T5R points on the EPIC-Kitchens-100 dataset.
                </p>
            </section>
            <!-- <p>
                RAFTformer achieves at least a 9x reduction in latency as well as a reduction of 94% in GPU training time and 90% 
                in the number of trainable parameters than prior state-of-the-art action forecasting methods. To the best of our 
                knowledge, our work is the first to achieve action anticipation in real-time (<i>i.e.</i> 25 fps).
            </p> -->
            <section id="BibTeX">
                <div class="container is-max-desktop content">
                    <h3 class="text-center mt-4 mb-2">
                        BibTeX
                    </h3>
                    <pre class="bibtex p-3"><code>@inproceedings{girase2023latency,
    title     = {Latency Matters: Real-Time Action Forecasting Transformer},
    author    = {Girase, Harshayu and Agarwal, Nakul and Choi, Chiho and Mangalam, Karttikeya},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages     = {18759--18769},
    year      = {2023}
}</code></pre>
                </div>
            </section>
        </div>
    </main>
</body>
</html>